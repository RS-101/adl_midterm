<!DOCTYPE html>
<html>
<head>
<title>overview.md</title>
<meta http-equiv="Content-type" content="text/html;charset=UTF-8">

<style>
/* https://github.com/microsoft/vscode/blob/master/extensions/markdown-language-features/media/markdown.css */
/*---------------------------------------------------------------------------------------------
 *  Copyright (c) Microsoft Corporation. All rights reserved.
 *  Licensed under the MIT License. See License.txt in the project root for license information.
 *--------------------------------------------------------------------------------------------*/

body {
	font-family: var(--vscode-markdown-font-family, -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif);
	font-size: var(--vscode-markdown-font-size, 14px);
	padding: 0 26px;
	line-height: var(--vscode-markdown-line-height, 22px);
	word-wrap: break-word;
}

#code-csp-warning {
	position: fixed;
	top: 0;
	right: 0;
	color: white;
	margin: 16px;
	text-align: center;
	font-size: 12px;
	font-family: sans-serif;
	background-color:#444444;
	cursor: pointer;
	padding: 6px;
	box-shadow: 1px 1px 1px rgba(0,0,0,.25);
}

#code-csp-warning:hover {
	text-decoration: none;
	background-color:#007acc;
	box-shadow: 2px 2px 2px rgba(0,0,0,.25);
}

body.scrollBeyondLastLine {
	margin-bottom: calc(100vh - 22px);
}

body.showEditorSelection .code-line {
	position: relative;
}

body.showEditorSelection .code-active-line:before,
body.showEditorSelection .code-line:hover:before {
	content: "";
	display: block;
	position: absolute;
	top: 0;
	left: -12px;
	height: 100%;
}

body.showEditorSelection li.code-active-line:before,
body.showEditorSelection li.code-line:hover:before {
	left: -30px;
}

.vscode-light.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(0, 0, 0, 0.15);
}

.vscode-light.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(0, 0, 0, 0.40);
}

.vscode-light.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-dark.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 255, 255, 0.4);
}

.vscode-dark.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 255, 255, 0.60);
}

.vscode-dark.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-high-contrast.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 160, 0, 0.7);
}

.vscode-high-contrast.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 160, 0, 1);
}

.vscode-high-contrast.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

img {
	max-width: 100%;
	max-height: 100%;
}

a {
	text-decoration: none;
}

a:hover {
	text-decoration: underline;
}

a:focus,
input:focus,
select:focus,
textarea:focus {
	outline: 1px solid -webkit-focus-ring-color;
	outline-offset: -1px;
}

hr {
	border: 0;
	height: 2px;
	border-bottom: 2px solid;
}

h1 {
	padding-bottom: 0.3em;
	line-height: 1.2;
	border-bottom-width: 1px;
	border-bottom-style: solid;
}

h1, h2, h3 {
	font-weight: normal;
}

table {
	border-collapse: collapse;
}

table > thead > tr > th {
	text-align: left;
	border-bottom: 1px solid;
}

table > thead > tr > th,
table > thead > tr > td,
table > tbody > tr > th,
table > tbody > tr > td {
	padding: 5px 10px;
}

table > tbody > tr + tr > td {
	border-top: 1px solid;
}

blockquote {
	margin: 0 7px 0 5px;
	padding: 0 16px 0 10px;
	border-left-width: 5px;
	border-left-style: solid;
}

code {
	font-family: Menlo, Monaco, Consolas, "Droid Sans Mono", "Courier New", monospace, "Droid Sans Fallback";
	font-size: 1em;
	line-height: 1.357em;
}

body.wordWrap pre {
	white-space: pre-wrap;
}

pre:not(.hljs),
pre.hljs code > div {
	padding: 16px;
	border-radius: 3px;
	overflow: auto;
}

pre code {
	color: var(--vscode-editor-foreground);
	tab-size: 4;
}

/** Theming */

.vscode-light pre {
	background-color: rgba(220, 220, 220, 0.4);
}

.vscode-dark pre {
	background-color: rgba(10, 10, 10, 0.4);
}

.vscode-high-contrast pre {
	background-color: rgb(0, 0, 0);
}

.vscode-high-contrast h1 {
	border-color: rgb(0, 0, 0);
}

.vscode-light table > thead > tr > th {
	border-color: rgba(0, 0, 0, 0.69);
}

.vscode-dark table > thead > tr > th {
	border-color: rgba(255, 255, 255, 0.69);
}

.vscode-light h1,
.vscode-light hr,
.vscode-light table > tbody > tr + tr > td {
	border-color: rgba(0, 0, 0, 0.18);
}

.vscode-dark h1,
.vscode-dark hr,
.vscode-dark table > tbody > tr + tr > td {
	border-color: rgba(255, 255, 255, 0.18);
}

</style>

<style>
/* Tomorrow Theme */
/* http://jmblog.github.com/color-themes-for-google-code-highlightjs */
/* Original theme - https://github.com/chriskempson/tomorrow-theme */

/* Tomorrow Comment */
.hljs-comment,
.hljs-quote {
	color: #8e908c;
}

/* Tomorrow Red */
.hljs-variable,
.hljs-template-variable,
.hljs-tag,
.hljs-name,
.hljs-selector-id,
.hljs-selector-class,
.hljs-regexp,
.hljs-deletion {
	color: #c82829;
}

/* Tomorrow Orange */
.hljs-number,
.hljs-built_in,
.hljs-builtin-name,
.hljs-literal,
.hljs-type,
.hljs-params,
.hljs-meta,
.hljs-link {
	color: #f5871f;
}

/* Tomorrow Yellow */
.hljs-attribute {
	color: #eab700;
}

/* Tomorrow Green */
.hljs-string,
.hljs-symbol,
.hljs-bullet,
.hljs-addition {
	color: #718c00;
}

/* Tomorrow Blue */
.hljs-title,
.hljs-section {
	color: #4271ae;
}

/* Tomorrow Purple */
.hljs-keyword,
.hljs-selector-tag {
	color: #8959a8;
}

.hljs {
	display: block;
	overflow-x: auto;
	color: #4d4d4c;
	padding: 0.5em;
}

.hljs-emphasis {
	font-style: italic;
}

.hljs-strong {
	font-weight: bold;
}
</style>

<style>
/*
 * Markdown PDF CSS
 */

 body {
	font-family: -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif, "Meiryo";
	padding: 0 12px;
}

pre {
	background-color: #f8f8f8;
	border: 1px solid #cccccc;
	border-radius: 3px;
	overflow-x: auto;
	white-space: pre-wrap;
	overflow-wrap: break-word;
}

pre:not(.hljs) {
	padding: 23px;
	line-height: 19px;
}

blockquote {
	background: rgba(127, 127, 127, 0.1);
	border-color: rgba(0, 122, 204, 0.5);
}

.emoji {
	height: 1.4em;
}

code {
	font-size: 14px;
	line-height: 19px;
}

/* for inline code */
:not(pre):not(.hljs) > code {
	color: #C9AE75; /* Change the old color so it seems less like an error */
	font-size: inherit;
}

/* Page Break : use <div class="page"/> to insert page break
-------------------------------------------------------- */
.page {
	page-break-after: always;
}

</style>

<script src="https://unpkg.com/mermaid/dist/mermaid.min.js"></script>
</head>
<body>
  <script>
    mermaid.initialize({
      startOnLoad: true,
      theme: document.body.classList.contains('vscode-dark') || document.body.classList.contains('vscode-high-contrast')
          ? 'dark'
          : 'default'
    });
  </script>
<h1 id="using-large-language-models-llms-for-text-summarization">Using Large Language Models (LLMs) for Text Summarization</h1>
<p><a href="https://github.com/RS-101/adl_midterm">Github repo</a></p>
<h2 id="introduction">Introduction</h2>
<p>This assignment explores text summarization using Large Language Models (LLMs) available through Hugging Face Transformers. The goal is to implement and evaluate an LLM-based summarization model for generating concise summaries from provided text data.</p>
<hr>
<h2 id="objective">Objective</h2>
<p>Utilize a generative AI model to create succinct summaries of given texts with Hugging Face Transformers.</p>
<h3 id="methodology">Methodology</h3>
<ol>
<li>Identify and select a summarization model from Hugging Face.</li>
<li>Implement a Python script using Hugging Face's pipeline to:
<ul>
<li>Accept a lengthy text input.</li>
<li>Generate a concise summary using the chosen AI model.</li>
<li>Display both original and summarized texts.</li>
</ul>
</li>
</ol>
<hr>
<h2 id="selected-model">Selected Model</h2>
<p>The chosen summarization models are:</p>
<ul>
<li>
<p><strong>Model Name:</strong> <a href="https://huggingface.co/facebook/bart-large-cnn">facebook/bart-large-cnn</a></p>
<ul>
<li><strong>Why?:</strong> Highest popularity on Hugging Face</li>
</ul>
</li>
<li>
<p><strong>Model Name:</strong> <a href="https://huggingface.co/sshleifer/distilbart-cnn-12-6">sshleifer/distilbart-cnn-12-6</a></p>
<ul>
<li><strong>Why?:</strong> Distilled version of BART and interesting to see comparison</li>
</ul>
</li>
<li>
<p><strong>Model Name:</strong> <a href="https://huggingface.co/google/pegasus-xsum">google/pegasus-xsum</a></p>
<ul>
<li><strong>Why?:</strong> Google is another big player in the AI space.</li>
</ul>
</li>
</ul>
<hr>
<h2 id="code-implementation">Code Implementation</h2>
<p>Below is the Python script used for generating summaries and benchmarking:</p>
<pre class="hljs"><code><div><span class="hljs-keyword">import</span> time
<span class="hljs-keyword">import</span> tracemalloc
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline

<span class="hljs-comment"># List of models to test</span>
MODEL_NAMES = [
    <span class="hljs-string">"google/pegasus-xsum"</span>,
    <span class="hljs-string">"facebook/bart-large-cnn"</span>,
    <span class="hljs-string">"sshleifer/distilbart-cnn-12-6"</span>
]

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">benchmark_summarization</span><span class="hljs-params">(text, max_length=<span class="hljs-number">100</span>, min_length=<span class="hljs-number">25</span>, batch_size=<span class="hljs-number">1</span>)</span>:</span>
    <span class="hljs-string">"""
    Benchmarks the execution time and memory usage of different summarization models.

    Parameters:
        text (str): The input text to summarize.
        max_length (int): Maximum length of summary.
        min_length (int): Minimum length of summary.
        batch_size (int): Batch size for the pipeline.

    Prints:
        A summary of performance metrics for each model.
    """</span>
    results = []

    <span class="hljs-keyword">for</span> model_name <span class="hljs-keyword">in</span> MODEL_NAMES:
        print(<span class="hljs-string">f"\nBenchmarking model: <span class="hljs-subst">{model_name}</span>"</span>)
        
        <span class="hljs-comment"># Load the summarization pipeline</span>
        summarizer = pipeline(<span class="hljs-string">"summarization"</span>, model=model_name, device=<span class="hljs-number">0</span>)  <span class="hljs-comment"># Use GPU if available</span>

        <span class="hljs-comment"># Start memory tracking</span>
        tracemalloc.start()
        
        <span class="hljs-comment"># Start time tracking</span>
        start_time = time.perf_counter()

        <span class="hljs-comment"># Run summarization</span>
        summary = summarizer(text, max_length=max_length, min_length=min_length, batch_size=batch_size)

        <span class="hljs-comment"># Stop time tracking</span>
        end_time = time.perf_counter()
        
        <span class="hljs-comment"># Stop memory tracking</span>
        current, peak = tracemalloc.get_traced_memory()
        tracemalloc.stop()

        <span class="hljs-comment"># Store results</span>
        results.append({
            <span class="hljs-string">"Model"</span>: model_name,
            <span class="hljs-string">"Execution Time (s)"</span>: end_time - start_time,
            <span class="hljs-string">"Memory Usage (MB)"</span>: peak / <span class="hljs-number">10</span>**<span class="hljs-number">6</span>,
            <span class="hljs-string">"Summary"</span>: summary[<span class="hljs-number">0</span>][<span class="hljs-string">'summary_text'</span>]
        })

    <span class="hljs-comment"># Print results</span>
    print(<span class="hljs-string">"\n===== Benchmarking Results ====="</span>)
    <span class="hljs-keyword">for</span> result <span class="hljs-keyword">in</span> results:
        print(<span class="hljs-string">f"Model: <span class="hljs-subst">{result[<span class="hljs-string">'Model'</span>]}</span>"</span>)
        print(<span class="hljs-string">f"Execution Time: <span class="hljs-subst">{result[<span class="hljs-string">'Execution Time (s)'</span>]:<span class="hljs-number">.4</span>f}</span> seconds"</span>)
        print(<span class="hljs-string">f"Memory Usage: <span class="hljs-subst">{result[<span class="hljs-string">'Memory Usage (MB)'</span>]:<span class="hljs-number">.2</span>f}</span> MB"</span>)
        print(<span class="hljs-string">f"Summary Output: <span class="hljs-subst">{result[<span class="hljs-string">'Summary'</span>]}</span>\n"</span>)

</div></code></pre>
<hr>
<h2 id="benchmarking-results">Benchmarking Results</h2>
<h3 id="case-1-news-article">Case 1: <a href="https://huggingface.co/facebook/bart-large-cnn">News article</a></h3>
<table>
<thead>
<tr>
<th>Model</th>
<th>Execution Time (s)</th>
<th>Memory Usage (MB)</th>
<th>Summary Output</th>
</tr>
</thead>
<tbody>
<tr>
<td>google/pegasus-xsum</td>
<td>7.5134</td>
<td>0.12</td>
<td>&quot;A New York woman who has been married 10 times is facing criminal charges for allegedly lying on a marriage license application, prosecutors say.&quot;</td>
</tr>
<tr>
<td>facebook/bart-large-cnn</td>
<td>6.6450</td>
<td>0.16</td>
<td>&quot;Liana Barrientos, 39, is charged with two counts of &quot;offering a false instrument for filing in the first degree&quot; In total, she has been married 10 times, with nine of her marriages occurring between 1999 and 2002. She is believed to still be married to four men.&quot;</td>
</tr>
<tr>
<td>sshleifer/distilbart-cnn-12-6</td>
<td>4.6523</td>
<td>0.10</td>
<td>&quot;Liana Barrientos, 39, is charged with two counts of &quot;offering a false instrument for filing in the first degree&quot; In total, she has been married 10 times, with nine of her marriages occurring between 1999 and 2002. At one time, she was married to eight men at once, prosecutors say.&quot;</td>
</tr>
</tbody>
</table>
<h3 id="case-2-first-paragraph-of-history-of-artificial-intelligence-ai">Case 2: First paragraph of <a href="https://www.britannica.com/science/history-of-artificial-intelligenc">History of artificial intelligence (AI)</a></h3>
<table>
<thead>
<tr>
<th>Model</th>
<th>Execution Time (s)</th>
<th>Memory Usage (MB)</th>
<th>Summary Output</th>
</tr>
</thead>
<tbody>
<tr>
<td>google/pegasus-xsum</td>
<td>6.4421</td>
<td>0.11</td>
<td>&quot;Alan Turing was one of the founding fathers of artificial intelligence (AI), and his ideas on machine intelligence are still relevant today.&quot;</td>
</tr>
<tr>
<td>facebook/bart-large-cnn</td>
<td>6.7917</td>
<td>0.17</td>
<td>&quot;During World War II Turing was a leading cryptanalyst at the Government Code and Cypher School in Bletchley Park, Buckinghamshire, England. During the war he gave considerable thought to the issue of machine intelligence. In 1948 he introduced many of the central concepts of AI in a report entitled “Intelligent Machinery”.&quot;</td>
</tr>
<tr>
<td>sshleifer/distilbart-cnn-12-6</td>
<td>3.9668</td>
<td>0.13</td>
<td>&quot;During World War II Turing was a leading cryptanalyst at the Government Code and Cypher School in Bletchley Park, Buckinghamshire, England. Turing could not turn to the project of building a stored-program electronic computing machine until the cessation of hostilities in Europe in 1945.&quot;</td>
</tr>
</tbody>
</table>
<h3 id="case-3-first-paragraph-of-evolutionary-medicine">Case 3: First paragraph of <a href="https://elifesciences.org/articles/69398">Evolutionary Medicine</a></h3>
<table>
<thead>
<tr>
<th>Model</th>
<th>Execution Time (s)</th>
<th>Memory Usage (MB)</th>
<th>Summary Output</th>
</tr>
</thead>
<tbody>
<tr>
<td>google/pegasus-xsum</td>
<td>7.0905</td>
<td>0.12</td>
<td>&quot;Evolutionary medicine aims to understand the forces that shape human health and disease, and how they might be affected in the future.&quot;</td>
</tr>
<tr>
<td>facebook/bart-large-cnn</td>
<td>4.9632</td>
<td>0.13</td>
<td>&quot;Our individual and collective health is shaped and affected by many factors. These factors include our environment, our inherited and somatic genetic variants, our diets and lifestyles. Human pathogens and parasites continually adapt to our biology and to cultural innovations.&quot;</td>
</tr>
<tr>
<td>sshleifer/distilbart-cnn-12-6</td>
<td>5.3459</td>
<td>0.13</td>
<td>&quot;Our individual and collective health is shaped and affected by many factors. These include our environment, our inherited and somatic genetic variants, our variable exposure to pathogens, our diets and lifestyles, our social systems, and our cultural innovations. Human genetic adaptations to our past environments, disease burdens, and cultural practices can affect disease risks today. Meanwhile, human pathogens and parasites continually adapt to our biology.&quot;</td>
</tr>
</tbody>
</table>
<hr>
<h2 id="conclusions">Conclusions</h2>
<p>We observed relatively long execution times across all three models, primarily due to running benchmarks on a CPU (Ryzen 7). However, memory usage remained low. The <code>google/pegasus-xsum</code> model consistently showed the longest execution time yet produced the shortest summaries. Conversely, both <code>facebook/bart-large-cnn</code> and its distilled version, <code>sshleifer/distilbart-cnn-12-6</code>, were generally faster and produced similar output. This similarity is expected since the distilled version is a compressed variant of the original model.</p>
<p>Given its shorter execution times and comparable output quality, the <code>sshleifer/distilbart-cnn-12-6</code> model appears preferable for CPU-based scenarios. Additional considerations for selecting models include their overall size and performance when executed on GPUs.</p>
<p><strong>Additional Note:</strong><br>
It's uncertain how effectively the current memory benchmarking results translate to GPU-based VRAM usage. Further GPU-based benchmarks would provide better insight into this aspect.</p>
<h2 id="included-prompts">Included Prompts</h2>
<p>ChatGPT 4o was used for writing and coding. With GPT initialized with the following prompts.</p>
<h3 id="initial-documentation-prompt">Initial Documentation Prompt</h3>
<p>The initial prompt for documenting this project was:</p>
<pre class="hljs"><code><div>You are an expert in Markdown formatting, technical summarization, and documentation. Your task is to generate a well-structured Markdown report for a small code assignment on using Large Language Models (LLMs) for text summarization.

Instructions:

Input: I will provide:
<span class="hljs-bullet">- </span>The task description
<span class="hljs-bullet">- </span>Code snippets
<span class="hljs-bullet">- </span>Results
<span class="hljs-bullet">- </span>Additional prompts that should be included in the documentation

Processing:
<span class="hljs-bullet">- </span>Summarize the purpose and methodology of the assignment.
<span class="hljs-bullet">- </span>Clearly document the provided code, explaining its functionality.
<span class="hljs-bullet">- </span>Summarize the results, highlighting key observations.
<span class="hljs-bullet">- </span>Incorporate any provided prompts into the Markdown output under a dedicated section.
<span class="hljs-bullet">- </span>Format the report in Markdown, using proper headings, code blocks, and bullet points.

Output: Return a well-structured Markdown document containing:
<span class="hljs-bullet">- </span>A title and brief introduction
<span class="hljs-bullet">- </span>Code explanation with proper formatting
<span class="hljs-bullet">- </span>A summary of results and conclusions
<span class="hljs-bullet">- </span>A section for included prompts and their purpose
<span class="hljs-bullet">- </span>Any relevant insights or next steps

Ensure that the output is clear, concise, and formatted for easy readability. Use fenced code blocks (<span class="hljs-code">```</span>) for code snippets and proper Markdown structure for clarity.
</div></code></pre>
<p><strong>Purpose of Included Prompt:</strong> To ensure clear, structured, and comprehensive documentation of the summarization task.</p>
<h3 id="python-command-line-expert-prompt">Python Command-Line Expert Prompt</h3>
<p>The initial prompt for code writing in this project was:</p>
<pre class="hljs"><code><div>*"You are an expert in Python 3.11 with deep knowledge of performance benchmarking for both execution time and memory usage. You are also proficient in the Hugging Face <span class="hljs-code">`pipeline`</span> package for NLP tasks. Your task is to analyze, optimize, and benchmark Python functions, ensuring efficient execution in terms of both speed and memory consumption.  

Given a Python function, provide:  
<span class="hljs-bullet">1. </span>A clear breakdown of potential performance bottlenecks.  
<span class="hljs-bullet">2. </span>Efficient benchmarking using modules like <span class="hljs-code">`timeit`</span>, <span class="hljs-code">`perf_counter`</span>, and <span class="hljs-code">`memory_profiler`</span>.  
<span class="hljs-bullet">3. </span>Optimized alternatives for improving speed and memory efficiency.  
<span class="hljs-bullet">4. </span>Best practices for integrating Hugging Face <span class="hljs-code">`pipeline`</span> efficiently within performance-critical applications.  

Ensure that your responses include clear, well-commented Python code, explanations of optimizations, and practical insights on real-world applications."*  
</div></code></pre>
<p><strong>Purpose of Included Prompt:</strong> To ensure structured, efficient, and professional guidance when developing Python command-line programs.</p>
<hr>

</body>
</html>
